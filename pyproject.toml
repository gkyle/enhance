[project]
name = "upscale"
version = "0.1.0"
description = "Enhance AI"
readme = "README.md"
requires-python = "==3.12.*"
dependencies = [
    "deferred-import>=0.1.0",
    "einops",
    "flash-attn", # https://github.com/Dao-AILab/flash-attention/issues/1696#issuecomment-2966762278
    "ipykernel>=6.29.5",
    "jsonpickle>=4.0.5",
    "numpy",
    "opencv-python-headless>=4.11.0.86",
    "pillow>=11.2.1",
    "pretrainedmodels>=0.7.4",
    "pyside6>=6.9.0",
    "sam-2",
    "spandrel>=0.4.1",
    "spandrel-extra-arches>=0.2.0",
    "tifftools>=1.6.0",
    "timm>=1.0.15",
    "transformers",
]


[project.optional-dependencies]
cpu = [
  "torch>=2.6.0",
  "torchvision",
]
cu118 = [
  "torch>=2.6.0",
  "torchvision",
]
cu126 = [
  "torch>=2.6.0",
  "torchvision",
]

[tool.uv]
no-build-isolation = true
conflicts = [
  [
    { extra = "cpu" },
    { extra = "cu118" },
    { extra = "cu126" },
  ],
]

[tool.uv.sources]
torch = [
  { index = "pytorch-cpu", extra = "cpu" },
  { index = "pytorch-cu118", extra = "cu118" },
  { index = "pytorch-cu126", extra = "cu126" },
]
sam-2 = { git = "https://github.com/facebookresearch/sam2.git", rev = "c2ec8e14a185632b0a5d8b161928ceb50197eddc" }
flash-attn = { git = "https://github.com/Dao-AILab/flash-attention.git" }

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[[tool.uv.index]]
name = "pytorch-cu118"
url = "https://download.pytorch.org/whl/cu118"
explicit = true

[[tool.uv.index]]
name = "pytorch-cu126"
url = "https://download.pytorch.org/whl/cu126"
explicit = true

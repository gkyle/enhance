[project]
name = "upscale"
version = "0.1.0"
description = "Enhance AI"
readme = "README.md"
requires-python = "==3.12.*"
dependencies = [
    "deferred-import>=0.1.0",
    "einops",
    "ipykernel>=6.29.5",
    "jsonpickle>=4.0.5",
    "numpy",
    "opencv-python-headless>=4.11.0.86",
    "pillow>=11.2.1",
    "pretrainedmodels>=0.7.4",
    "pyside6>=6.9.0",
    "spandrel>=0.4.1",
    "spandrel-extra-arches>=0.2.0",
    "tifftools>=1.6.0",
    "timm>=1.0.15",
    "transformers",
]

[project.optional-dependencies]
cpu = [
  "torch>=2.6.0",
  "torchvision",
]
cu118 = [
  "torch>=2.6.0",
  "torchvision",
]
cu126 = [
  "torch>=2.6.0",
  "torchvision",
]
detect = [
  "flash-attn==2.7.4",
  "sam-2",
  "triton-windows>=3.3.1.post19; platform_system == 'Windows'",
]

[tool.uv]
no-build-isolation = true
conflicts = [
  [
    { extra = "cpu" },
    { extra = "cu118" },
    { extra = "cu126" },
  ],
]

[tool.uv.sources]
torch = [
  { index = "pytorch-cpu", extra = "cpu" },
  { index = "pytorch-cu118", extra = "cu118" },
  { index = "pytorch-cu126", extra = "cu126" },
]
sam-2 = { git = "https://github.com/facebookresearch/sam2.git", rev = "c2ec8e14a185632b0a5d8b161928ceb50197eddc" }
# For windows, prefer a prebuilt flash_attn wheel. https://huggingface.co/lldacing/flash-attention-windows-wheel/tree/main
flash-attn = [
  {url = "https://huggingface.co/lldacing/flash-attention-windows-wheel/resolve/main/flash_attn-2.7.4%2Bcu126torch2.6.0cxx11abiFALSE-cp312-cp312-win_amd64.whl?download=true", marker = "platform_system == 'Windows'"},
  {git = "https://github.com/Dao-AILab/flash-attention", tag = "v2.7.4", marker = "platform_system != 'Windows'"}
]

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[[tool.uv.index]]
name = "pytorch-cu118"
url = "https://download.pytorch.org/whl/cu118"
explicit = true

[[tool.uv.index]]
name = "pytorch-cu126"
url = "https://download.pytorch.org/whl/cu124"
explicit = true

